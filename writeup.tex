\documentclass{article}
\usepackage{amsmath}
\begin{document}

\noindent Jack Lanchantin\\
CS 6501 HW2

\begin{enumerate}

\item[1.1]
\begin{align*}
	J(\beta) & = (y-X\beta)^2 + \lambda\beta\beta \\
	& = (y-X\beta)^T(y-X\beta) + \beta^T(\lambda I)\beta \\
	& = y^Ty - 2yX\beta + X^TX\beta^T\beta + \beta^T(\lambda I)\beta \\
			\\
	\nabla_\beta J(\beta) & = -2y^TX + 2X^TX\beta + 2\lambda I \beta \\
	0 & = -2y^TX + \beta(2X^TX + 2\lambda I) \\
	2y^TX & = \beta(2X^TX + 2\lambda I) \\
	\beta & = (X^TX + \lambda I)^{-1} y^TX 
\end{align*}

\item[1.2]
\begin{align*}
	X & =
	\begin{bmatrix}
	  1 & 2 \\
	  3 & 6 \\
	  5 & 10
	\end{bmatrix}, 
	\,Y = 
	\begin{bmatrix}
	  1 & 2 & 3 \\
	\end{bmatrix}^T \\
	\\
	\theta & = (X^TX)^{-1}X^Ty \\ \\
	X^TX & = 
	\begin{bmatrix}
	  5 & 15 & 25 \\
	  15 & 45 & 75\\
	  25 & 75 & 125
	\end{bmatrix} \longrightarrow non\,\,invertable \\
\end{align*}
=$>$ Cannot be solved by linear regression because $X^TX$ is non-invertable\\

\item[1.3]
Lasso Regression should be used because it can generate a sparse $\beta$ vector\\

\item[1.5]
Ridge regression performs better on this data set. This is probably due to the fact that after running linear regression between $x_1$ and $x_2$, I found that $\theta$ $\approx$ 2. This means that the features are not linearly independent ($x_2$ $\approx$ 2$x_1$), and thus the $X^TX$ matrix is non invertable, so using $\lambda$I in the ridge regression to regularize the function makes it so that the $X^TX$ matrix is guaranteed to be invertable, resulting in a more accurate $\beta$.

\end{enumerate}
\end{document}